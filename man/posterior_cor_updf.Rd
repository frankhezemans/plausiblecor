% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/posterior.R
\name{posterior_cor_updf}
\alias{posterior_cor_updf}
\title{Approximation of Posterior Density Function for Correlation Coefficient}
\usage{
posterior_cor_updf(
  r,
  n,
  kappa = NULL,
  alternative = c("two.sided", "greater", "less"),
  method = c("pearson", "kendall"),
  n_bins = 1000,
  max_iter = 1e+07
)
}
\arguments{
\item{r}{Numeric value. The observed sample correlation coefficient, must be
between \code{-1} and \code{1}.}

\item{n}{Numeric value. The sample size, must be at least \code{3}.}

\item{kappa}{Numeric value. Parameter controlling the "concentration" of the
stretched beta prior on the correlation coefficient (see details
below). If \code{NULL} (default), it is set to a value that induces a
uniform prior; namely \code{1} for \code{method = "pearson"} and \code{2} for
\code{method = "kendall"}.}

\item{alternative}{Character string specifying the alternative hypothesis:
\describe{
\item{"two.sided"}{ (default) Prior is supported on \eqn{\left[-1, 1\right]}}
\item{"greater"}{ Prior truncated to \eqn{\left[0, 1\right]} and re-normalised}
\item{"less"}{ Prior truncated to \eqn{\left[-1, 0\right]} and re-normalised}
}}

\item{method}{Character string specifying which correlation coefficient is to
be used for the test. One of \code{"pearson"} (default) or \code{"kendall"}.}

\item{n_bins}{Integer. Number of grid points for the approximation, default
is \code{1e3}.}

\item{max_iter}{Integer. Maximum number of iterations (attempts) to solve
generalised hypergeometric functions that are necessary to compute
the posterior density for \code{method = "pearson"}. Default is \code{1e7}. See
details.}
}
\value{
A function that evaluates the unnormalized posterior density at any
correlation value between \code{-1} and \code{1}. The function is proportional
to the true posterior density but may not integrate to 1 due to
interpolation.
}
\description{
This function calculates an approximation to the posterior density of
Pearson's correlation coefficient (Ly et al., 2018) or Kendall's rank
correlation coefficient (Van Doorn et al., 2018). The result is returned as a
function that is proportional to the posterior density (i.e., an
\emph{unnormalised}) posterior density function), constructed via linear
interpolation between grid points.
}
\details{
The probability density of the posterior distribution of the correlation
coefficient is computed for a discretised grid of values. Then,
linear interpolation between these density values is used to construct an
approximation to the probability density \emph{function} of the posterior, using
\code{\link[stats:approxfun]{stats::approxfun()}}.

The discrete grid of correlation values is set adaptively, so that there are
many estimates of the density near the posterior mode (i.e., the observed
correlation) and near the endpoints of +1 and -1.

For \code{method = "pearson"}, computing the posterior density involves several
evaluations of the generalised hypergeometric function, which is handled
internally by \code{\link[hypergeo:genhypergeo]{hypergeo::genhypergeo()}}. In some cases, especially when the
observed correlation approaches +1 or -1, \code{\link[hypergeo:genhypergeo]{hypergeo::genhypergeo()}} may need
several thousand attempts to find the solution. Here, we cautiously set the
default \code{max_iter} to \code{1e7}, but note that the original default in
\code{\link[hypergeo:genhypergeo]{hypergeo::genhypergeo()}} is much lower (\code{2e3}).

For \code{method = "kendall"}, the posterior density is based on the asymptotic
normal approximation to the sampling distribution of Kendall's rank
correlation coefficient, tau (see Van Doorn, 2018, and references therein
for details). In practice, the quality of this normal approximation depends
on both the value of the observed tau \code{r} and the sample size \code{n}: the
approximation improves exponentially as \code{n} increases, but larger values of
\code{r} require larger \code{n} to achieve the same accuracy. Users should be cautious
when applying this method to small samples and/or strong correlations.
A warning is issued if the given values of \code{r} and \code{n} fail to meet
rule-of-thumb criteria based on simulation results from Van Doorn et al. (2018).

Any undefined or non-finite estimate of the posterior density is treated as
\code{NA} and ignored by \code{\link[stats:approxfun]{stats::approxfun()}} when constructing the function.

The prior is a stretched beta distribution with shape parameters
\eqn{\alpha = \beta = \frac{1}{\kappa}}, scaled to the interval (-1, 1). This
creates a symmetric distribution centered at zero and its domain stretched to
cover the full range of the correlation coefficient. The prior can optionally
also be truncated (and re-normalised) to support a directional
(i.e., non-negative or non-positive) alternative hypothesis.

This function was primarily based on code that was originally written by
Alexander Ly, adapted by Dora Matzke, and then released with the Dynamic
Models of Choice toolbox (Heathcote et al., 2019). Furthermore, code for
Kendall's rank correlation was adapted from code written by Alexander Ly and
Johnny van Doorn that was released with the \code{bstats} R package (which is used
under the hood by the stand-alone statistics programme JASP).
}
\examples{
# Posterior for Pearson's correlation coefficient --------------------------

# example: correlation between cars' horsepower and quarter mile race time
# (will be strongly negative: cars with more HP tend to need less time)
r <- cor(mtcars$hp, mtcars$qsec)
n <- nrow(mtcars)
post_fun <- posterior_cor_updf(r, n)
# this function can be used to obtain the (unnormalised) posterior density
# of a correlation value
post_fun(r)
post_fun(c(-0.25, 0.25))
# for visualisation, obtain the posterior density for an equally-spaced grid
# of correlation values
grid_res <- 1e-3
grid <- seq(from = -1, to = 1, by = grid_res)
post_dens <- post_fun(grid)
plot(
  x = grid, y = post_dens, type = "l",
  xlab = "Pearson correlation", ylab = "",
  bty = "n", yaxt = "n"
)

# for inference, we first need to normalise the grid of posterior densities
normalise <- function(dens, dx = grid_res) {
  return(dens / sum(dens * dx))
}
post_dens <- normalise(post_dens)
post_mode <- grid[which.max(post_dens)]
post_median <- approx(
  x = cumsum(post_dens) * grid_res,
  y = grid,
  xout = 0.5,
  ties = "ordered"
)[["y"]]
post_mean <- sum(grid * post_dens) * grid_res

# Directional hypotheses ---------------------------------------------------

# the alternative hypothesis can be two-sided or one-sided (strictly non-
# negative or strictly non-positive).
# example: more ambiguous correlation (cars' weight and quarter mile time)
r_weak <- cor(mtcars$wt, mtcars$qsec)
post_fun <- posterior_cor_updf(r_weak, n)
# specify hypothesis that correlation is strictly non-positive
post_fun_lower <- posterior_cor_updf(r_weak, n, alternative = "less")
# compare posterior distributions for two-sided and one-sided hypotheses
plot(
  x = grid, y = normalise(post_fun_lower(grid)), type = "l", col = "red",
  xlab = "Pearson correlation", ylab = "",
  bty = "n", yaxt = "n"
)
lines(x = grid, y = normalise(post_fun(grid)))

# Kendall's rank order correlation coefficient -----------------------------

# Kendall's correlation is invariant to monotonic (rank-preserving)
# transformations of data, as it is based on ranks rather than values
# example: cars' HP is positively skewed (most cars with low-to-moderate HP,
# some cars with high HP); use square root to transform
r_pearson <- cor(mtcars$hp, mtcars$drat)
r_pearson_trans <- cor(sqrt(mtcars$hp), mtcars$drat)
identical(r_pearson, r_pearson_trans)
r_kendall <- cor(mtcars$hp, mtcars$drat, method = "kendall")
r_kendall_trans <- cor(sqrt(mtcars$hp), mtcars$drat, method = "kendall")
identical(r_kendall, r_kendall_trans)

post_fun_pearson <- posterior_cor_updf(r_pearson, n)
post_fun_pearson_trans <- posterior_cor_updf(r_pearson_trans, n)
post_fun_kendall <- posterior_cor_updf(r_kendall, n, method = "kendall")
post_fun_kendall_trans <- posterior_cor_updf(r_kendall_trans, n, method = "kendall")

# the square-root transformation affects inference for Pearson's correlation
# but not for Kendall's correlation
original_par <- par(no.readonly = TRUE)
par(mfrow = c(1, 2))
plot(
  x = grid, y = normalise(post_fun_pearson_trans(grid)),
  type = "l", col = "red",
  xlab = "Pearson correlation", ylab = "",
  bty = "n", yaxt = "n"
)
lines(x = grid, y = normalise(post_fun_pearson(grid)))
plot(
  x = grid, y = normalise(post_fun_kendall(grid)), type = "l",
  xlab = "Kendall correlation", ylab = "",
  bty = "n", yaxt = "n"
)
lines(x = grid, y = normalise(post_fun_kendall_trans(grid)), col = "red")
par(original_par)

}
\references{
Heathcote, A., Lin, Y.S., Reynolds, A., Strickland, L., Gretton, M., &
Matzke, D. (2019). Dynamic models of choice. \emph{Behavior Research Methods},
\emph{51}, 961-985. \doi{10.3758/s13428-018-1067-y}

Ly, A., Marsman, M., & Wagenmakers, E.-J. (2018). Analytic posteriors for
Pearson's correlation coefficient. \emph{Statistica Neerlandica}, \emph{72}, 4â€“13.
\doi{10.1111/stan.12111}

Van Doorn, J., Ly, A., Marsman, M., & Wagenmakers, E.-J. (2018). Bayesian
inference for Kendall's rank correlation coefficient. \emph{The American Statistician},
\emph{72}, 303-308. \doi{10.1080/00031305.2016.1264998}
}
